
# Module 1 Learning Objectives
By the end of this module, you should be able to meet the following learning objectives: 

## Lesson 1: The K-Armed Bandit Problem

- Define reward

- Understand the temporal nature of the bandit problem

- Define k-armed bandit

- Define action-values

## Lesson 2: What to Learn? Estimating Action Values

- Define action-value estimation methods

- Define exploration and exploitation

- Select actions greedily using an action-value function

- Define online learning

- Understand a simple online sample-average action-value estimation method

- Define the general online update equation

- Understand why we might use a constant step-size in the case of non-stationarity

 

## Lesson 3: Exploration vs. Exploitation Tradeoff 

- Define epsilon-greedy

- Compare the short-term benefits of exploitation and the long-term benefits of exploration

- Understand optimistic initial values

- Describe the benefits of optimistic initial values for early exploration

- Explain the criticisms of optimistic initial values

- Describe the upper confidence bound action selection method

- Define optimism in the face of uncertainty

# Module 2 Learning Objectives
By the end of this module, you should have achieved the following learning objectives: 

## Lesson 1: Introduction to Markov Decision Processes  
 

- Understand Markov Decision Processes, or MDPs
 

- Describe how the dynamics of an MDP are defined
 

- Understand the graphical representation of a Markov Decision Process
 

- Explain how many diverse processes can be written in terms of the MDP framework
 
 

## Lesson 2: Goal of Reinforcement Learning   

- Describe how rewards relate to the goal of an agent
 

- Understand episodes and identify episodic tasks
 
 

## Lesson 3: Continuing Tasks
 

- Formulate returns for continuing tasks using discounting
 
- Describe how returns at successive time steps are related to each other
 

- Understand when to formalize a task as episodic or continuing

# Module 3 Learning Objectives
By the end of this lesson, you should have achieved the following learning objectives: 

## Lesson 1: Policies and Value Functions

- Recognize that a policy is a distribution over actions for each possible state
 

- Describe the similarities and differences between stochastic and deterministic policies
 

- Identify the characteristics of a well-defined policy
 

- Generate examples of valid policies for a given MDP
 

- Describe the roles of state-value and action-value functions in reinforcement learning
 

- Describe the relationship between value functions and policies
 

- Create examples of valid value functions for a given MDP
 
 

## Lesson 2: Bellman Equations

- Derive the Bellman equation for state-value functions
 

- Derive the Bellman equation for action-value functions
 

- Understand how Bellman equations relate current and future values
 

- Use the Bellman equations to compute value functions
 
 

## Lesson 3: Optimality (Optimal Policies & Value Functions)

- Define an optimal policy
 

- Understand how a policy can be at least as good as every other policy in every state
 

- Identify an optimal policy for given MDPs
 

- Derive the Bellman optimality equation for state-value functions
 

- Derive the Bellman optimality equation for action-value functions
 

- Understand how the Bellman optimality equations relate to the previously introduced Bellman equations
 

- Understand the connection between the optimal value function and optimal policies
 

- Verify the optimal value function for given MDPs